{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9TnLRamjdTbGTESaPaUK4",
      "include_colab_link": false
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "HcArGmJmUPBs"
      },
      "outputs": [],
      "source": [
        "import scipy as sp\n",
        "import numpy as np\n",
        "from sympy.abc import x, y\n",
        "from sympy import Abs\n",
        "import sympy\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Метод Ньютона\n",
        "\n",
        "**Метод Ньютона** – метод поиска минимума функции нескольких переменных. Его особенностью является то, что направление спуска зависит от свойств минимизируемой функции (используются вторые производные).\n",
        "Используя модели для приближения функции (похожие в окрестности функции), в нашем случае используя ряд Тейлора до второй включительно производной, можно выразить $f_{k + 1}$ через $f_k$ и при условии минимума получить формулу для шага:\n",
        "\\begin{align}\n",
        "p_{(k)} = hessian(f_{(k)})^{-1} \\cdot grad(f_{(k)})\n",
        "\\end{align}\n",
        "\n",
        "Для получения градиента использовалась функция *sp.optimize.approx_fprime*.\n",
        "\n",
        "Для получения гессиана использовалась функция *sympy.hessian*.\n",
        "\n",
        "## Используемые методы одномерной оптимизации:\n",
        "1. **Метод золотого сечения** (см. metopt1)\n",
        "2. **Шаг, длина которого зависит от свойств минимизируемой функции**\n",
        "Можно не менять направление до тех пор, пока функция $f(x)$ в направлении $p_{(k)}$ убывает. Тогда будем двигаться по направлению убывания до его минимума – точки x_{(k + 1)}. В ней же определим новое направление движения, ортогональное текущему.\n",
        "С помощью моделей для приближения функции получаем следующую формулу для длины шага:\n",
        "\\begin{align}\n",
        "\\alpha = - \\frac {grad(f_{(k)})^{Τ} \\cdot p_{(k)}} { p_{(k)}^{T} \\cdot hessian(f_{(k)}) \\cdot p_{(k)}}\n",
        "\\end{align}\n",
        "\n",
        "3. **Правило Вольфе**   \n",
        "Возьмем касательную в нашей точке и приподнимем ее на коэффициент $c_1$, значение которого около единицы, но меньше ее. Полученная прямая предположительно пересекает нашу функцию в точке, в которой значение функции уже возрастает. Следовательно, искомая следующая точка лежит в этом промежутке.  \n",
        "Возьмем теперь касательную с коэффициентом $c_2$, большим $0$, но около него.Заданная точка предположительно должна находится на промежутке убывания функции, однако уже ближе к минимуму, и, следовательно, с более ‘пологой’ касательной. Итого, наклон касательной в текущей точке должен быть резче чем полученный наклон.\n",
        "Эти два рассуждения задают два условия.  \n",
        "1) Условие Армихо\n",
        "\\begin{align}\n",
        "f(x_{(k)} + α \\cdot p_{(k)})  - f(x_{(k)}) \\le c1 \\cdot \\alpha \\cdot \\langle grad( f(x_{(k)}) ),  p_{(k)}\\rangle\n",
        "\\end{align}\n",
        "2) Условие кривизны\n",
        "\\begin{align}\n",
        "\\langle grad( f(x_{(k)} + α \\cdot p_{(k)})),  p_{(k)}\\rangle \\ge c_2 \\cdot \\langle grad( f(x_{(k)}) ),  p_{(k)}\\rangle\n",
        "\\end{align}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xi_Vm00UUbca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def constant_step(step_size, x_last, p, f_runnable, eps, gradient, hessian):\n",
        "    '''The function returns a constant specified step'''\n",
        "    return np.array([step_size, 0, 0]);\n",
        "\n",
        "def golden_step(step_size, x_last, p, f_runnable, eps, gradient, hessian):\n",
        "    '''The function returns the step calculated using the golden ratio method'''\n",
        "    phi = (1 + 5 ** 0.5) / 2\n",
        "    cnt_func = 0\n",
        "    a = 0\n",
        "    b = 1\n",
        "    ml = b - (b - a) / phi\n",
        "    mr = a + (b - a) / phi\n",
        "    xl = [x_i + ml * p_i[0] for x_i, p_i in zip(x_last, p)]\n",
        "    xr = [x_i + mr * p_i[0] for x_i, p_i in zip(x_last, p)]\n",
        "    lvalue = f_runnable(xl)\n",
        "    rvalue = f_runnable(xr)\n",
        "    cnt_func += 2\n",
        "\n",
        "    while (b - a > eps):\n",
        "      if lvalue < rvalue:\n",
        "        b = mr\n",
        "        mr = ml\n",
        "        ml = b - (b - a) / phi\n",
        "        rvalue = lvalue\n",
        "        xr = xl\n",
        "        xl = [x_i + ml * p_i[0] for x_i, p_i in zip(x_last, p)]\n",
        "        lvalue = f_runnable(xl)\n",
        "        cnt_func += 1\n",
        "      else:\n",
        "        a = ml\n",
        "        ml = mr\n",
        "        mr = a + (b - a) / phi\n",
        "        lvalue = rvalue\n",
        "        xl = xr\n",
        "        xr = [x_i + mr * p_i[0] for x_i, p_i in zip(x_last, p)]\n",
        "        rvalue = f_runnable(xr)\n",
        "        cnt_func += 1\n",
        "    return np.array([a, cnt_func, 0]);\n",
        "\n",
        "def best_step(step_size, x_last, p, f_runnable, eps, gradient, hessian):\n",
        "  '''The function returns a step whose value depends on the properties of the minimized function'''\n",
        "  numerator = np.dot(gradient.transpose(), p)\n",
        "  denominator = np.dot(np.dot(p.transpose(), hessian), p)\n",
        "  res = -1 * numerator[0][0] / denominator[0][0]\n",
        "  return np.array([res, 0, 0]);\n",
        "\n",
        "def step_Wolfe_rule(step_size, x_last, p, f_runnable, eps, gradient, hessian):\n",
        "  '''The function returns the step calculated using Wolfe's rule'''\n",
        "  e1 = 0.8\n",
        "  e2 = 0.2\n",
        "  l = 0.8\n",
        "  a = 1\n",
        "  cnt_func = 1\n",
        "  cnt_grad = 0\n",
        "  f_last = f_runnable(x_last)\n",
        "\n",
        "  x_new = [x + a * p_i[0] for x, p_i in zip(x_last, p)]\n",
        "  diff1 = f_runnable(x_new) - f_last\n",
        "  cnt_func += 1\n",
        "\n",
        "  gradient_x_new = sp.optimize.approx_fprime(x_new, f_runnable, 1e-6).reshape((2, 1))\n",
        "  diff2 = np.dot(np.squeeze(np.asarray(gradient_x_new)), np.squeeze(np.asarray(p)))\n",
        "  cnt_grad += 1\n",
        "\n",
        "  tangent = np.dot(np.squeeze(np.asarray(gradient)), np.squeeze(np.asarray(p)))\n",
        "  l1 = e1 * a * tangent\n",
        "  l2 = e2 * tangent\n",
        "\n",
        "  while ((diff1 - l1 > eps) and (diff2 - l2 >= eps)):\n",
        "    a = l * a\n",
        "\n",
        "    x_new = [x + a * p_i[0] for x, p_i in zip(x_last, p)]\n",
        "    diff1 = f_runnable(x_new) - f_last\n",
        "    cnt_func += 1\n",
        "\n",
        "    gradient_x_new = sp.optimize.approx_fprime(x_new, f_runnable, 1e-6).reshape((2, 1))\n",
        "    diff2 = np.dot(np.squeeze(np.asarray(gradient_x_new)), np.squeeze(np.asarray(p)))\n",
        "    cnt_grad += 1\n",
        "\n",
        "    tangent = np.dot(np.squeeze(np.asarray(gradient)), np.squeeze(np.asarray(p)))\n",
        "    l1 = e1 * a * tangent\n",
        "    l2 = e2 * tangent\n",
        "\n",
        "  return np.array([a, cnt_func, cnt_grad]);"
      ],
      "metadata": {
        "id": "Cf73J-YJblgS"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_golden_ratio(f, x0, eps):\n",
        "  x_last = x0\n",
        "  x_res = x0\n",
        "  phi = (1 + 5 ** 0.5) / 2\n",
        "  cnt_grad = 0\n",
        "  cnt_func = 0\n",
        "  cnt_iter = 0\n",
        "\n",
        "  while True:\n",
        "    cnt_iter += 1\n",
        "\n",
        "    cnt_grad += 1\n",
        "    grad = sp.optimize.approx_fprime(x_last, f, 1e-6)\n",
        "\n",
        "    a = 0\n",
        "    b = 1\n",
        "    ml = b - (b - a) / phi\n",
        "    mr = a + (b - a) / phi\n",
        "    xl = [x - ml * grad_i for x, grad_i in zip(x_last, grad)]\n",
        "    xr = [x - mr * grad_i for x, grad_i in zip(x_last, grad)]\n",
        "    lvalue = f(xl)\n",
        "    rvalue = f(xr)\n",
        "    cnt_func += 2\n",
        "\n",
        "    while (b - a > eps):\n",
        "      if lvalue < rvalue:\n",
        "        b = mr\n",
        "        mr = ml\n",
        "        ml = b - (b - a) / phi\n",
        "        rvalue = lvalue\n",
        "        xr = xl\n",
        "        xl = [x - ml * grad_i for x, grad_i in zip(x_last, grad)]\n",
        "        lvalue = f(xl)\n",
        "        cnt_func += 1\n",
        "      else:\n",
        "        a = ml\n",
        "        ml = mr\n",
        "        mr = a + (b - a) / phi\n",
        "        lvalue = rvalue\n",
        "        xl = xr\n",
        "        xr = [x - mr * grad_i for x, grad_i in zip(x_last, grad)]\n",
        "        rvalue = f(xr)\n",
        "        cnt_func += 1\n",
        "\n",
        "    x_new = [x - a * grad_i for x, grad_i in zip(x_last, grad)]\n",
        "\n",
        "    cnt_func += 2\n",
        "    if abs(f(x_new) - f(x_last)) < eps:\n",
        "      break;\n",
        "\n",
        "    x_last = x_res\n",
        "    x_res = x_new\n",
        "\n",
        "  res = np.array([x_res[0], x_res[1], f(x_res), cnt_iter, cnt_grad, cnt_func])\n",
        "  return res;"
      ],
      "metadata": {
        "id": "VmQKW0GpdPeD"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import LinAlgError\n",
        "\n",
        "def Newton_method(f_runnable, f_sympy, f_step, x0, eps, step_size):\n",
        "    \"\"\"Newton method.\n",
        "\n",
        "    Returns a numpy array containing:\n",
        "    the x coordinates (at which the minimum is reached),\n",
        "    the minimum value,\n",
        "    the number of iterations,\n",
        "    the number of gradient and hessian calculations,\n",
        "    the number of function calculations.\n",
        "    \"\"\"\n",
        "    x_res = x0\n",
        "    x_last = x0\n",
        "    cnt_grad_and_hess = 0\n",
        "    cnt_func = 0\n",
        "    cnt_iter = 0\n",
        "    hess = sympy.hessian(f_sympy, (x, y))\n",
        "\n",
        "    while True:\n",
        "      cnt_iter += 1\n",
        "\n",
        "      cnt_grad_and_hess += 1\n",
        "      gradient = sp.optimize.approx_fprime(x_last, f_runnable, 1e-6).reshape((len(x0), 1))\n",
        "      hessian = hess.subs(x, x_last[0]).subs(y, x_last[1])\n",
        "\n",
        "      if hessian.det() ==  0.0:\n",
        "        print(\"Erorr: Matrix det == 0. Current function: \",  f_sympy, \", current arguments: \", x_last[0], x_last[1])\n",
        "        print(\"Calculations continue using gradient descent.\")\n",
        "        print()\n",
        "        return gradient_descent_golden_ratio(f_runnable, x_last, eps);\n",
        "      else:\n",
        "        p = np.asarray(-((hessian ** - 1) * gradient))\n",
        "\n",
        "      eval = f_step(step_size, x_last, p, f_runnable, eps, gradient, hessian)\n",
        "      step = eval[0]\n",
        "      cnt_func += eval[1]\n",
        "      cnt_grad_and_hess += eval[2]\n",
        "      x_new = [x_i + step * p_i[0] for x_i, p_i in zip(x_last, p)]\n",
        "\n",
        "      cnt_func += 2\n",
        "      if abs(f_runnable(x_new) - f_runnable(x_last)) < eps:\n",
        "        break;\n",
        "\n",
        "      x_last = x_res\n",
        "      x_res = x_new\n",
        "\n",
        "    res = np.array([*x_res, f_runnable(x_res), cnt_iter, cnt_grad_and_hess, cnt_func])\n",
        "    return res;"
      ],
      "metadata": {
        "id": "lapT375Ccm-1"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import autograd.numpy as anp\n",
        "from autograd import grad, jacobian\n",
        "\n",
        "# Nelder-Mead\n",
        "def Nelder_Mead_from_scipy_optimize(f_runnable, x0, eps):\n",
        "   result = minimize(f_runnable, x0, method='Nelder-Mead')\n",
        "   return np.array([*result.x, result.fun, result.nit, 0, result.nfev])\n",
        "\n",
        "# Метод Ньютона с постоянным шагом\n",
        "def Newton_constant_step_method(f_runnable, f_sympy, x0, step_size, eps):\n",
        "    return Newton_method(f_runnable, f_sympy, constant_step, x0, eps, step_size)\n",
        "\n",
        "# Метод Ньютона с одномерным поиском (метод золотого сечения)\n",
        "def Newton_golden_ratio_method(f_runnable, f_sympy, x0, eps):\n",
        "    return Newton_method(f_runnable, f_sympy, golden_step, x0, eps, 0)\n",
        "\n",
        "# Метод Ньютона с шагом, длина которого зависит от свойств минимизируемой функции\n",
        "def Newton_best_step_method(f_runnable, f_sympy, x0, eps):\n",
        "    return Newton_method(f_runnable, f_sympy, best_step, x0, eps, 0)\n",
        "\n",
        "# Метод Ньютона реализованный в библиотеке scipy.optimize\n",
        "def Newton__method_from_scipy_optimize(f_runnable, x0, eps):\n",
        "   jacobian_f = jacobian(f_runnable)\n",
        "   result = minimize(f_runnable, x0, method='Newton-CG', jac=jacobian_f)\n",
        "   return np.array([*result.x, result.fun, result.nit, result.njev, result.nfev])\n",
        "\n",
        "# Квази-ньютоновский метод из scipy.optimize с вычислением градиента через разностные схемы\n",
        "def Quasi_Newton_with_finite_differential_scheme_from_scipy_optimize(f_runnable, x0, eps):\n",
        "    result = minimize(f_runnable, x0, method='BFGS')\n",
        "    return np.array([*result.x, result.fun, result.nit, result.njev, result.nfev])\n",
        "\n",
        "def Quasi_Newton_with_analitical_grad_from_scipy_optimize(f_runnable, f_grad, x0, eps):\n",
        "    result = minimize(f_runnable, x0, method='BFGS', jac=f_grad)\n",
        "    return np.array([*result.x, result.fun, result.nit, result.njev, result.nfev])\n",
        "\n",
        "# Метод Ньютона с одномерным поиском (правило Вольфе)\n",
        "def Newton_method_Wolfe_rule(f_runnable, f_sympy, x0, eps):\n",
        "    return Newton_method(f_runnable, f_sympy, step_Wolfe_rule, x0, eps, 0)\n"
      ],
      "metadata": {
        "id": "XG-5qZwndgq3"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def message(result, len_x):\n",
        "  print(\"----- Argument values = [\", \", \".join(str(result[i]) for i in range(len_x)), \"]\")\n",
        "  print(\"----- Function result = \", result[len_x])\n",
        "  print(\"----- Iteration count = \", result[len_x + 1])\n",
        "  print(\"----- Gradient evaluation count = \", result[len_x + 2])\n",
        "  print(\"----- Function evaluation count = \", result[len_x + 3])\n",
        "  print()\n",
        "\n",
        "def test(start_point, f_runnable, f_sympy, const_step, eps):\n",
        "  len_x = len(start_point)\n",
        "\n",
        "  print(\"Function: \" + str(f_sympy))\n",
        "  print(\"Starting point: \" + str(start_point))\n",
        "  print(\"Const step: \", const_step)\n",
        "  print(\"Required accuracy : \", eps)\n",
        "  print()\n",
        "\n",
        "  constant_step_res = Newton_constant_step_method(f_runnable, f_sympy, start_point, const_step, eps)\n",
        "  print(\"Newton method with constant step\")\n",
        "  message(constant_step_res, len_x)\n",
        "\n",
        "  golden_ratio_res = Newton_golden_ratio_method(f_runnable, f_sympy, start_point, eps)\n",
        "  print(\"Newton method with golden ratio\")\n",
        "  message(golden_ratio_res, len_x)\n",
        "\n",
        "  best_step_res = Newton_best_step_method(f_runnable, f_sympy, start_point, eps)\n",
        "  print(\"Newton method with best step function\")\n",
        "  message(best_step_res, len_x)\n",
        "\n",
        "  scipy_res = Newton__method_from_scipy_optimize(f_runnable, start_point, eps)\n",
        "  print(\"Newton method from scipy optimize\")\n",
        "  message(scipy_res, len_x)\n",
        "\n",
        "def test_main(start_point, f_runnable, f_sympy, const_step, eps, Nelder_Mead=True, gradient=True, Newton=True, Quasi_Newton=True):\n",
        "  len_x = len(start_point)\n",
        "\n",
        "  print(\"Function: \" + str(f_sympy))\n",
        "  print(\"Starting point: \" + str(start_point))\n",
        "  print(\"Const step: \", const_step)\n",
        "  print(\"Required accuracy : \", eps)\n",
        "  print()\n",
        "  if Nelder_Mead:\n",
        "    print(\"--Нулевой порядок--\")\n",
        "\n",
        "    nelder_mead_res = Nelder_Mead_from_scipy_optimize(f_runnable, start_point, eps)\n",
        "    print(\"Nelder-Mead from scipy optimize\")\n",
        "    message(nelder_mead_res, len_x)\n",
        "\n",
        "  if gradient:\n",
        "    print(\"--Градиент--\")\n",
        "\n",
        "    gradient_golden_ratio_res = gradient_descent_golden_ratio(f_runnable, start_point, eps)\n",
        "    print(\"Gradient descent with golden ratio\")\n",
        "    message(gradient_golden_ratio_res, len_x)\n",
        "\n",
        "  if Newton:\n",
        "    print(\"--Реализации метода Ньютона--\")\n",
        "\n",
        "    constant_step_res = Newton_constant_step_method(f_runnable, f_sympy, start_point, const_step, eps)\n",
        "    print(\"Newton method with constant step\")\n",
        "    message(constant_step_res, len_x)\n",
        "\n",
        "    golden_ratio_res = Newton_golden_ratio_method(f_runnable, f_sympy, start_point, eps)\n",
        "    print(\"Newton method with golden ratio\")\n",
        "    message(golden_ratio_res, len_x)\n",
        "\n",
        "    best_step_res = Newton_best_step_method(f_runnable, f_sympy, start_point, eps)\n",
        "    print(\"Newton method with best step function\")\n",
        "    message(best_step_res, len_x)\n",
        "\n",
        "    scipy_res = Newton__method_from_scipy_optimize(f_runnable, start_point, eps)\n",
        "    print(\"Newton method from scipy optimize\")\n",
        "    message(scipy_res, len_x)\n",
        "\n",
        "  if Quasi_Newton:\n",
        "    print(\"--Квази-ньютоновские реализации--\")\n",
        "\n",
        "    quasi_newton_scipy_res1 = Quasi_Newton_with_finite_differential_scheme_from_scipy_optimize(f_runnable, start_point, eps)\n",
        "    print(\"Quasi-Newton from scipy optimize (finite differential scheme)\")\n",
        "    message(quasi_newton_scipy_res1, len_x)\n",
        "\n",
        "\n",
        "def test_dop2(start_point, f_runnable, f_sympy, const_step, eps):\n",
        "  len_x = len(start_point)\n",
        "  print(\"Function: \" + str(f_sympy))\n",
        "  print(\"Starting point: \" + str(start_point))\n",
        "  print(\"Const step: \", const_step)\n",
        "  print(\"Required accuracy : \", eps)\n",
        "  print()\n",
        "\n",
        "  Wolfe_rule_res = Newton_method_Wolfe_rule(f_runnable, f_sympy, start_point, eps)\n",
        "  print(\"Newton method with Wolfe rule\")\n",
        "  message(Wolfe_rule_res, len_x)\n",
        "\n",
        "  golden_ratio_res = Newton_golden_ratio_method(f_runnable, f_sympy, start_point, eps)\n",
        "  print(\"Newton method with golden ratio\")\n",
        "  message(golden_ratio_res, len_x)\n",
        "\n",
        "  scipy_res = Newton__method_from_scipy_optimize(f_runnable, start_point, eps)\n",
        "  print(\"Newton method from scipy optimize\")\n",
        "  message(scipy_res, len_x)\n"
      ],
      "metadata": {
        "id": "2d9X1xI_eREV"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func1_s = x ** 4 + y ** 2 - x * y\n",
        "\n",
        "def func1_g(x):\n",
        "  return [4 * x[0] ** 3 - x[1], -x[0] + 2 * x[1]]\n",
        "\n",
        "def func1_r(x):\n",
        "  return x[0] ** 4 + x[1] ** 2 - x[0] * x[1]\n",
        "\n",
        "test_main([-10, 10], func1_r, func1_s, 0.3, 1e-6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxsAbbrcfCe8",
        "outputId": "0b02da96-96b4-4352-fd76-20fc43ec5749"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function: x**4 - x*y + y**2\n",
            "Starting point: [-10, 10]\n",
            "Const step:  0.3\n",
            "Required accuracy :  1e-06\n",
            "\n",
            "--Нулевой порядок--\n",
            "Nelder-Mead from scipy optimize\n",
            "----- Argument values = [ -0.35355178412397703, -0.17675569473594704 ]\n",
            "----- Function result =  -0.015624999590777656\n",
            "----- Iteration count =  58.0\n",
            "----- Gradient evaluation count =  0.0\n",
            "----- Function evaluation count =  110.0\n",
            "\n",
            "--Градиент--\n",
            "Gradient descent with golden ratio\n",
            "----- Argument values = [ 0.3541794983286121, 0.17771121806651025 ]\n",
            "----- Function result =  -0.015624417423695733\n",
            "----- Iteration count =  27.0\n",
            "----- Gradient evaluation count =  27.0\n",
            "----- Function evaluation count =  891.0\n",
            "\n",
            "--Реализации метода Ньютона--\n",
            "Newton method with constant step\n",
            "----- Argument values = [ -0.355235853687952, -0.177616821739200 ]\n",
            "----- Function result =  -0.0156235779145158\n",
            "----- Iteration count =  91\n",
            "----- Gradient evaluation count =  91.0\n",
            "----- Function evaluation count =  182.0\n",
            "\n",
            "Newton method with golden ratio\n",
            "----- Argument values = [ -0.353631747406693, -0.176816373701972 ]\n",
            "----- Function result =  -0.0156249969291745\n",
            "----- Iteration count =  23\n",
            "----- Gradient evaluation count =  23.0\n",
            "----- Function evaluation count =  759.0\n",
            "\n",
            "Newton method with best step function\n",
            "----- Argument values = [ -0.353631738413825, -0.176816369202791 ]\n",
            "----- Function result =  -0.0156249969298793\n",
            "----- Iteration count =  23\n",
            "----- Gradient evaluation count =  23\n",
            "----- Function evaluation count =  46\n",
            "\n",
            "Newton method from scipy optimize\n",
            "----- Argument values = [ -0.3535533918358628, -0.17677669547566782 ]\n",
            "----- Function result =  -0.015624999999999993\n",
            "----- Iteration count =  12.0\n",
            "----- Gradient evaluation count =  38.0\n",
            "----- Function evaluation count =  13.0\n",
            "\n",
            "--Квази-ньютоновские реализации--\n",
            "Quasi-Newton from scipy optimize (finite differential scheme)\n",
            "----- Argument values = [ -0.3535603683283055, -0.17678080472958063 ]\n",
            "----- Function result =  -0.015624999975270025\n",
            "----- Iteration count =  17.0\n",
            "----- Gradient evaluation count =  18.0\n",
            "----- Function evaluation count =  54.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скорость примерно совпадает, как и точность"
      ],
      "metadata": {
        "id": "9akMxyUri43i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция Розенброка\n",
        "func2_s = (1 - x) ** 2 + 100 * (y - x ** 2) ** 2\n",
        "\n",
        "def func2_r(x):\n",
        "  return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n",
        "\n",
        "test_main([-100, 10], func2_r, func2_s, 0.3, 1e-6, gradient=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGnLp9Knfbp_",
        "outputId": "36db5e40-32f0-46e0-b0bd-af245521453f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function: (1 - x)**2 + 100*(-x**2 + y)**2\n",
            "Starting point: [-100, 10]\n",
            "Const step:  0.3\n",
            "Required accuracy :  1e-06\n",
            "\n",
            "--Нулевой порядок--\n",
            "Nelder-Mead from scipy optimize\n",
            "----- Argument values = [ 0.9999869896858198, 0.9999708843923565 ]\n",
            "----- Function result =  1.1272627305581398e-09\n",
            "----- Iteration count =  155.0\n",
            "----- Gradient evaluation count =  0.0\n",
            "----- Function evaluation count =  289.0\n",
            "\n",
            "--Реализации метода Ньютона--\n",
            "Newton method with constant step\n",
            "----- Argument values = [ 0.998643051507238, 0.997251492692670 ]\n",
            "----- Function result =  1.97418135240291e-6\n",
            "----- Iteration count =  951\n",
            "----- Gradient evaluation count =  951.0\n",
            "----- Function evaluation count =  1902.0\n",
            "\n",
            "Newton method with golden ratio\n",
            "----- Argument values = [ 0.999388375216632, 0.998768662577219 ]\n",
            "----- Function result =  3.81245320043937e-7\n",
            "----- Iteration count =  431\n",
            "----- Gradient evaluation count =  431.0\n",
            "----- Function evaluation count =  14223.0\n",
            "\n",
            "Newton method with best step function\n",
            "----- Argument values = [ 0.999699718145827, 0.999399026450308 ]\n",
            "----- Function result =  9.01941929994418e-8\n",
            "----- Iteration count =  61\n",
            "----- Gradient evaluation count =  61\n",
            "----- Function evaluation count =  122\n",
            "\n",
            "Newton method from scipy optimize\n",
            "----- Argument values = [ -3.26270999429421, 10.65184282200628 ]\n",
            "----- Function result =  18.17500814490616\n",
            "----- Iteration count =  400.0\n",
            "----- Gradient evaluation count =  1189.0\n",
            "----- Function evaluation count =  401.0\n",
            "\n",
            "--Квази-ньютоновские реализации--\n",
            "Quasi-Newton from scipy optimize (finite differential scheme)\n",
            "----- Argument values = [ 0.9999954996450119, 0.9999909914971781 ]\n",
            "----- Function result =  2.025929947054561e-11\n",
            "----- Iteration count =  389.0\n",
            "----- Gradient evaluation count =  498.0\n",
            "----- Function evaluation count =  1494.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Первые две реализации метода Ньютона гораздо медленнее третьей, а библиотечная вообще не попадает. Но если поменять стартовую точку на другую относительно банана, то получится"
      ],
      "metadata": {
        "id": "pwNU64kwjIWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция Розенброка\n",
        "func2_s = (1 - x) ** 2 + 100 * (y - x ** 2) ** 2\n",
        "\n",
        "def func2_r(x):\n",
        "  return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n",
        "\n",
        "test_main([10, -10], func2_r, func2_s, 0.3, 1e-6, gradient=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Xqw4u4wfkOH",
        "outputId": "4b965781-cb89-4aae-9c01-1b1051b510d3"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function: (1 - x)**2 + 100*(-x**2 + y)**2\n",
            "Starting point: [10, -10]\n",
            "Const step:  0.3\n",
            "Required accuracy :  1e-06\n",
            "\n",
            "--Нулевой порядок--\n",
            "Nelder-Mead from scipy optimize\n",
            "----- Argument values = [ 1.0000236025024942, 1.000045308074558 ]\n",
            "----- Function result =  9.171240085468826e-10\n",
            "----- Iteration count =  86.0\n",
            "----- Gradient evaluation count =  0.0\n",
            "----- Function evaluation count =  162.0\n",
            "\n",
            "--Реализации метода Ньютона--\n",
            "Newton method with constant step\n",
            "----- Argument values = [ 1.00110718085400, 1.00216724475841 ]\n",
            "----- Function result =  1.45955206532435e-6\n",
            "----- Iteration count =  243\n",
            "----- Gradient evaluation count =  243.0\n",
            "----- Function evaluation count =  486.0\n",
            "\n",
            "Newton method with golden ratio\n",
            "----- Argument values = [ 0.999980277504155, 0.999951591864825 ]\n",
            "----- Function result =  8.42346826275461e-9\n",
            "----- Iteration count =  93\n",
            "----- Gradient evaluation count =  93.0\n",
            "----- Function evaluation count =  3069.0\n",
            "\n",
            "Newton method with best step function\n",
            "----- Argument values = [ 0.999700064567785, 0.999399719060814 ]\n",
            "----- Function result =  8.99862670998858e-8\n",
            "----- Iteration count =  11\n",
            "----- Gradient evaluation count =  11\n",
            "----- Function evaluation count =  22\n",
            "\n",
            "Newton method from scipy optimize\n",
            "----- Argument values = [ 0.9999913347150167, 0.9999826349753403 ]\n",
            "----- Function result =  7.520639441408052e-11\n",
            "----- Iteration count =  41.0\n",
            "----- Gradient evaluation count =  139.0\n",
            "----- Function evaluation count =  50.0\n",
            "\n",
            "--Квази-ньютоновские реализации--\n",
            "Quasi-Newton from scipy optimize (finite differential scheme)\n",
            "----- Argument values = [ 0.9999932691680833, 0.9999865406980754 ]\n",
            "----- Function result =  4.530463495676315e-11\n",
            "----- Iteration count =  85.0\n",
            "----- Gradient evaluation count =  112.0\n",
            "----- Function evaluation count =  336.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все попали, но третья реализация была быстрее всех. Точность +- одинаковая"
      ],
      "metadata": {
        "id": "ZajaIjRAjSTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "func3_s = -((1/(x ** 2 + 0.3)) ** 2 + (1/(y ** 2 + 0.3)) ** 2)\n",
        "\n",
        "def func3_r(x):\n",
        "  return -((1/(x[0] ** 2 + 0.3)) ** 2 + (1/(x[1] ** 2 + 0.3)) ** 2)\n",
        "\n",
        "test_main([-10, 10], func3_r, func3_s, 0.3, 1e-6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JClSrDj2flzF",
        "outputId": "076a4907-e728-452a-d8c1-907137fac381"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function: -1/(y**2 + 0.3)**2 - 1/(x**2 + 0.3)**2\n",
            "Starting point: [-10, 10]\n",
            "Const step:  0.3\n",
            "Required accuracy :  1e-06\n",
            "\n",
            "--Нулевой порядок--\n",
            "Nelder-Mead from scipy optimize\n",
            "----- Argument values = [ -5.473005825176055e-06, -0.00022611300968056224 ]\n",
            "----- Function result =  -22.222218432812305\n",
            "----- Iteration count =  78.0\n",
            "----- Gradient evaluation count =  0.0\n",
            "----- Function evaluation count =  144.0\n",
            "\n",
            "--Градиент--\n",
            "Gradient descent with golden ratio\n",
            "----- Argument values = [ -10.0, 10.0 ]\n",
            "----- Function result =  -0.00019880537848070942\n",
            "----- Iteration count =  1.0\n",
            "----- Gradient evaluation count =  1.0\n",
            "----- Function evaluation count =  33.0\n",
            "\n",
            "--Реализации метода Ньютона--\n",
            "Newton method with constant step\n",
            "----- Argument values = [ -25.4432727951513, 25.4432650993168 ]\n",
            "----- Function result =  -4.76799654679420e-6\n",
            "----- Iteration count =  33\n",
            "----- Gradient evaluation count =  33.0\n",
            "----- Function evaluation count =  66.0\n",
            "\n",
            "Newton method with golden ratio\n",
            "----- Argument values = [ -10.0, 10.0 ]\n",
            "----- Function result =  -0.00019880537848070942\n",
            "----- Iteration count =  1.0\n",
            "----- Gradient evaluation count =  1.0\n",
            "----- Function evaluation count =  33.0\n",
            "\n",
            "Newton method with best step function\n",
            "----- Argument values = [ -35.8966733863607, 35.8966604545117 ]\n",
            "----- Function result =  -1.20395788803185e-6\n",
            "----- Iteration count =  15\n",
            "----- Gradient evaluation count =  15\n",
            "----- Function evaluation count =  30\n",
            "\n",
            "Newton method from scipy optimize\n",
            "----- Argument values = [ 1.5706637820054898e-18, -1.5706637820054898e-18 ]\n",
            "----- Function result =  -22.222222222222225\n",
            "----- Iteration count =  2.0\n",
            "----- Gradient evaluation count =  11.0\n",
            "----- Function evaluation count =  9.0\n",
            "\n",
            "--Квази-ньютоновские реализации--\n",
            "Quasi-Newton from scipy optimize (finite differential scheme)\n",
            "----- Argument values = [ -1.0866507693663152e-10, 1.0866507693663152e-10 ]\n",
            "----- Function result =  -22.222222222222225\n",
            "----- Iteration count =  1.0\n",
            "----- Gradient evaluation count =  20.0\n",
            "----- Function evaluation count =  60.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тут наоборот самописные методы промахиваются, а библиотечный хорошо себя чувствует"
      ],
      "metadata": {
        "id": "VyoaLjRojYNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Выводы\n",
        "\n",
        "1. В среднем метод Ньютона с шагом\n",
        "\\begin{align}\n",
        "\\alpha = - \\frac {grad(f_{(k)})^{Τ} \\cdot p_{(k)}} { p_{(k)}^{T} \\cdot hessian(f_{(k)}) \\cdot p_{(k)}}\n",
        "\\end{align}\n",
        "работает лучше других наших реализаций: количество итераций и вычислений функции, производных меньше, в некоторых случаях даже на порядок.\n",
        "2. Существуют функции, для некоторых точек которых, не справляется либо наша реализация, либо библиотечная. Мы не смогли объяснить природу этих результатов, но пришли к выводу, что при необходимости найти минимум стоит использовать несколько реализаций.\n"
      ],
      "metadata": {
        "id": "SLSF3D67keHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Допольнительное задание 1\n",
        "\n",
        "Задача: Реализовать одномерный поиск по правилу Вольфе (Wolfe) и метод Ньютона на его\n",
        "основе и исследовать на эффективность (сравнить с реализованным методами Ньютона и методом Newton-CG)."
      ],
      "metadata": {
        "id": "bNsN7CppkiOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Дополнительное задание\n",
        "\n",
        "func1_s = (1 - x) ** 2 + 100 * (y - x ** 2) ** 2\n",
        "def func1_r(x):\n",
        "  return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n",
        "\n",
        "test_dop2([10, -10], func1_r, func1_s, 0.5, 1e-9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-8FJUCwfqHS",
        "outputId": "a5f601d6-0fbf-400c-c3dd-6e9170136ef7"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function: (1 - x)**2 + 100*(-x**2 + y)**2\n",
            "Starting point: [10, -10]\n",
            "Const step:  0.5\n",
            "Required accuracy :  1e-09\n",
            "\n",
            "Newton method with Wolfe rule\n",
            "----- Argument values = [ 0.999699647202244, 0.999398884082153 ]\n",
            "----- Function result =  9.02368565620529e-8\n",
            "----- Iteration count =  105\n",
            "----- Gradient evaluation count =  270.0\n",
            "----- Function evaluation count =  480.0\n",
            "\n",
            "Newton method with golden ratio\n",
            "----- Argument values = [ 0.999980300056983, 0.999951658974757 ]\n",
            "----- Function result =  8.38317879384241e-9\n",
            "----- Iteration count =  93\n",
            "----- Gradient evaluation count =  93.0\n",
            "----- Function evaluation count =  4464.0\n",
            "\n",
            "Newton method from scipy optimize\n",
            "----- Argument values = [ 0.9999913347150167, 0.9999826349753403 ]\n",
            "----- Function result =  7.520639441408052e-11\n",
            "----- Iteration count =  41.0\n",
            "----- Gradient evaluation count =  139.0\n",
            "----- Function evaluation count =  50.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "func5_s = (x - 1) * (x - 3) * (x - 5) * (x - 8) / 100 + y ** 2\n",
        "def func5_r(X):\n",
        "    return (X[0] - 1) * (X[0] - 3) * (X[0] - 5) * (X[0] - 8) / 100 + X[1] * X[1]\n",
        "\n",
        "test_dop2([3, -10], func5_r, func5_s, 0.01, 1e-9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDnxvziUgEAo",
        "outputId": "de517967-3005-4e98-ccd1-69bb9c7bd771"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function: y**2 + (x - 8)*(x - 5)*(x - 3)*(x - 1)/100\n",
            "Starting point: [3, -10]\n",
            "Const step:  0.01\n",
            "Required accuracy :  1e-09\n",
            "\n",
            "Newton method with Wolfe rule\n",
            "----- Argument values = [ 4.03830052664243, -2.60999966054070e-5 ]\n",
            "----- Function result =  0.120191764680093\n",
            "----- Iteration count =  17\n",
            "----- Gradient evaluation count =  50.0\n",
            "----- Function evaluation count =  84.0\n",
            "\n",
            "Newton method with golden ratio\n",
            "----- Argument values = [ 1.77633647374038, -5.00350038857234e-7 ]\n",
            "----- Function result =  -0.190593381958226\n",
            "----- Iteration count =  5\n",
            "----- Gradient evaluation count =  5.0\n",
            "----- Function evaluation count =  240.0\n",
            "\n",
            "Newton method from scipy optimize\n",
            "----- Argument values = [ 1.7763364813673486, -9.219441067073002e-13 ]\n",
            "----- Function result =  -0.1905933819584762\n",
            "----- Iteration count =  6.0\n",
            "----- Gradient evaluation count =  18.0\n",
            "----- Function evaluation count =  8.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вывод\n",
        "Метод Ньютона с правилом Вольфе работает примерно аналогично методу Ньютона с золотым сечением: они оба ищут оптимальную длину шага, с которым двигаться по направлению убывания. Поэтому у них аналогичные траектории и примерно одинаковое количество итераций. Однако, во-первых, количество вычислений функции при методе золотого сечения на порядок больше, чем при правиле Вольфе; но количество вычислений градиента при методе золотого сечения в среднем в три раза меньше, чем чем при правиле Вольфе. Во-вторых, обе реализации проигрывают по всем параметрам библиотечной реализации. Итого, стоит понимать, какое из вычисление более трудозатратное – вычисление самой функции или вичисление ее производных, и выбирать соответствующий метод поиска."
      ],
      "metadata": {
        "id": "rRr9yQ9Xlvwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Дополнительное задание 2\n",
        "В реализацию метода Ньютона была добавлена проверка на нулевой определитель гессиана. В этом случае выводится сообщение об ошибке и вычисление продолжается с помощью градиентного спуска."
      ],
      "metadata": {
        "id": "BMAKuyNCmMzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для дополнительного задания 2, часть 1\n",
        "func_dop2_1_s = (x + y) ** 2\n",
        "def func_dop2_1_r(x):\n",
        "  return (x[0] + x[1]) ** 2\n",
        "\n",
        "test([10, 10], func_dop2_1_r, func_dop2_1_s, 0.5, 1e-6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niviw8QNgGYf",
        "outputId": "cae4ad49-cd79-4a7d-ca3a-ca2a0294be22"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function: (x + y)**2\n",
            "Starting point: [10, 10]\n",
            "Const step:  0.5\n",
            "Required accuracy :  1e-06\n",
            "\n",
            "Erorr: Matrix det == 0. Current function:  (x + y)**2 , current arguments:  10 10\n",
            "Calculations continue using gradient descent.\n",
            "\n",
            "Newton method with constant step\n",
            "----- Argument values = [ 1.8424369526925943e-05, 1.8424369526925943e-05 ]\n",
            "----- Function result =  1.3578295698588691e-09\n",
            "----- Iteration count =  3.0\n",
            "----- Gradient evaluation count =  3.0\n",
            "----- Function evaluation count =  99.0\n",
            "\n",
            "Erorr: Matrix det == 0. Current function:  (x + y)**2 , current arguments:  10 10\n",
            "Calculations continue using gradient descent.\n",
            "\n",
            "Newton method with golden ratio\n",
            "----- Argument values = [ 1.8424369526925943e-05, 1.8424369526925943e-05 ]\n",
            "----- Function result =  1.3578295698588691e-09\n",
            "----- Iteration count =  3.0\n",
            "----- Gradient evaluation count =  3.0\n",
            "----- Function evaluation count =  99.0\n",
            "\n",
            "Erorr: Matrix det == 0. Current function:  (x + y)**2 , current arguments:  10 10\n",
            "Calculations continue using gradient descent.\n",
            "\n",
            "Newton method with best step function\n",
            "----- Argument values = [ 1.8424369526925943e-05, 1.8424369526925943e-05 ]\n",
            "----- Function result =  1.3578295698588691e-09\n",
            "----- Iteration count =  3.0\n",
            "----- Gradient evaluation count =  3.0\n",
            "----- Function evaluation count =  99.0\n",
            "\n",
            "Newton method from scipy optimize\n",
            "----- Argument values = [ 0.0, 0.0 ]\n",
            "----- Function result =  0.0\n",
            "----- Iteration count =  2.0\n",
            "----- Gradient evaluation count =  3.0\n",
            "----- Function evaluation count =  2.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1lylknXpga45"
      }
    }
  ]
}
