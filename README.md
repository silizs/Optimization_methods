# Optimization_methods
Лабораторные работы, выполненные в рамках курса «Методы оптимизации»

1. Методы нулевого порядка
2. Методы первого и высших порядков
3. Метод стохастического градиентного спуска (SGD) и его модификации
4. Методы стохастической оптимизации. Настройка гиперпараметров *(выложу, если не забуду)*

В первых двух лабораторных были реализованы метод градиентного спуска, метод Ньютона (а так же метод дихотомии, метод золотого сечения, условие Вольфе), проводились сравнения с методами из библиотеки scipy.optimize (по точности и скорости) и анализ.

В третьей лабораторной работе был реализован стохастический градиентный спуск с разными размерами батча (от одного до размера полной коллекции), с разными функциями изменения шага - learning rate scheduling (основанный на времени темп обучения, экспоненциальный темп обучения), с добавлением регуляризации (L1, L2, Elastic регуляризации). Изучены scipy.optimize: SGD, и модификации SGD (Nesterov, Momentum, AdaGrad, RMSProp, Adam). Проведено исследование на эффективность для решения линейной регрессии. 

В четвертой лабораторной работе реализован метод имитации отжига, применены методы библиотеки optuna к выделенным hyperпараметрам из лаб. 2.

