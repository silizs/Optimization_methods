{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqUF/Jc2WIiG6GqG4CPDJx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "G-T6TUnhxRRw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import math\n",
        "import random\n",
        "from typing import Callable\n",
        "import itertools\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Описание алгоритмов\n",
        "\n",
        "## SGD (Stochastic gradient descent)\n",
        "Cтохастический градиентный спуск – метод поиска минимума для функций вида $f = f1 + f2 + ... +  fn$.    \n",
        "В отличие от обычного градиентного спуска, в SGD градиент будет браться от случайного слагаемого функции, а не от ее всей.    \n",
        "Общая задача:\n",
        "\\begin{align}\n",
        "        \\sum_{i=1}^n L_i(w) + λ \\cdot R(w) → min.\n",
        "    \\end{align}\n",
        "где   \n",
        "1. $w$ - параметры, которые мы обучаем;    \n",
        "2. $L$ - lost function - ошибка нашей системы на конкретном экземпляре;       \n",
        "3. сумма идет по всей базе данных, на которой мы обучаем. База данных выглядит следующим образом $\\{x(i), y(i)\\}$, где $x(i)$ - запрос, $y(i)$ - правильный ответ;    \n",
        "4. $R$ - регуляризация, какие-то дополнительные ограничения (например, норма вектора, если мы хотим минимизировать параметры).\n",
        "\n",
        "### Алгоритм:\n",
        "Фиксируем 2 гипер-параметра: $h$ - темп обучения, $λ$ - темп забывания.\n",
        "Считаем:\n",
        "1. $w(0)$ - начальное значение параметров;   \n",
        "2. $Q = \\frac{1}{m} \\cdot \\sum_{k=1}^m L_{j_k}(w^{(0)}) $ - средняя ошибка в начале по какой-то выборке (возможно по всей)   \n",
        "\n",
        "На итерации $t$ эпохи обучения $e$ (эпоха заканчивается, когда мы обработали все $L_i$):\n",
        "1. выбираем новый уникальный $i$;\n",
        "2. вычисляем $grad(L_i(w^{(t)}))$, обновляем параметры\n",
        "\\begin{align}\n",
        "        w^{(t + 1)} = w^{(t)} - h \\cdot grad(L_i(w^{(t)}))\n",
        "    \\end{align}\n",
        "3. вычисляем $\\epsilon_i = L_i(w^{(t)})$, обновляем значение ошибки\n",
        "\\begin{align}\n",
        "       Q = λ \\cdot ϵ_i + (1 - λ) \\cdot Q\n",
        "    \\end{align}\n",
        "Если $Q$ сходится, завершаем выполнение.\n",
        "\n",
        "\n",
        "В случае **mini-batch SGD** на одной итерации обрабатывается множество $\\{ L_i \\}$, размера batchSize.\n",
        "\n",
        "**Затухание** – **attenuationParameter** –  служит для остановки обучения в хорошем месте и избежания колебаний – ситуации, которая может возникнуть, когда из-за слишком высокого постоянного темпа обучения происходит перепрыгивание через минимум.\n",
        "О функциях изменения шага (learning rate scheduling):\n",
        "1. шаг может быть постоянным на протяжении всего обучения;\n",
        "2. основанный на времени темп обучения\n",
        "\\begin{align}\n",
        "       h_{n + 1} = h_n / (1 + d \\cdot n)\n",
        "\\end{align}\n",
        "где $h$ - темп обучения, $d$ - параметр затухания, $n$ - шаг итерации;\n",
        "3. экспоненциальный темп обучения\n",
        "\\begin{align}\n",
        "       h_{n} = h_0 \\cdot e^{- dn}\n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "       h_{n + 1} = h_n \\cdot e^{- d}\n",
        "\\end{align}\n"
      ],
      "metadata": {
        "id": "U4cObME4xsI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Реализация алгоритмов\n",
        "\n",
        "В реализации использовался метод наименьших квадратов.\n",
        "То есть функция потерь (целевая функция) выглядит так:\n",
        "\\begin{align}\n",
        "       L_i = \\sum_{i} L_{(i)} = \\sum_{i} (y_{(i)} - \\widehat{y_{(i)}})^2 = \\sum_{i} (y_{(i)} - w \\times x_{(i)})^2\n",
        "\\end{align}\n",
        "где $w$ - параметры, которые обучаем, $\\times$ - скалярное произведение векторов.    \n",
        "А градиент $L_{(i)}$  так:\n",
        "\\begin{align}\n",
        "       grad(L_{(i)}) = 2 \\cdot x_{(i)} \\cdot (w \\times x_{(i)} - y_{(i)})\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Функция data_fabric по значениям data_size(int) и coefficients(vector) генерирует линейную регрессию (которую далее мы пытаемся восстановить) –\n",
        "множество размера data_size и вида $\\{ x(i), y(i) \\}$, где $y_{(i)}$ – значение,  $x_{(i)}$ – вектор:\n",
        "\\begin{align}\n",
        "       y_{(i)} = \\sum_{k = 0}^{len(coefficients)} coefficients_k \\cdot randomNumber =  \\sum_{k = 0}^{len(coefficients)} x_{(i)_k}\n",
        "\\end{align}\n"
      ],
      "metadata": {
        "id": "QBGFLv4V4ojV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функции для генерации линейной регрессии вида: y = coef1 * (x1 ** deg1) + ... coefn * (xn ** degn)\n",
        "\n",
        "def generate(coefficients: np.array,\n",
        "             degrees: np.array):\n",
        "    n = len(coefficients)\n",
        "    x = np.random.rand(n)\n",
        "    y = sum((pow(x[i], degrees[i]) * coefficients[i]) for i in range(n))\n",
        "    return [x, y]\n",
        "\n",
        "def data_fabric(data_size: int,\n",
        "                coefficients: np.array,\n",
        "                degrees: np.array):\n",
        "    x = np.array([])\n",
        "    y = np.array([])\n",
        "    for i in range(data_size):\n",
        "        new_data = generate(coefficients, degrees)\n",
        "        x = np.append(x, new_data[0])\n",
        "        y = np.append(y, new_data[1])\n",
        "    x = x.reshape(data_size, len(coefficients))\n",
        "    return [x, y]"
      ],
      "metadata": {
        "id": "bI4XsVaR6zls"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Реализация SGD - основное задание\n",
        "\n",
        "def stochastic_gradient_descent(X: np.array,\n",
        "                                Y: np.array,\n",
        "                                forgetting_rate: float,\n",
        "                                learning_rate_function: Callable[[float, float, int], float],\n",
        "                                initial_value_learning_rate: float,\n",
        "                                attenuation_parameter: float,\n",
        "                                batch_size: int = 1,\n",
        "                                eps: float = 1e-6,\n",
        "                                ):\n",
        "    data_size, number_coefficients = X.shape\n",
        "    W = np.zeros(number_coefficients)\n",
        "\n",
        "    Q_last = sum(((Y[i] - np.dot(W, X[i])) ** 2) / data_size for i in range(data_size))\n",
        "    Q_new = Q_last\n",
        "\n",
        "    h = initial_value_learning_rate\n",
        "    cnt_epoch = 0\n",
        "    cnt_iter = 0\n",
        "    while True:\n",
        "        list_indexes = list(range(0, data_size))\n",
        "\n",
        "        for i in range(math.ceil(data_size / batch_size)):\n",
        "            random_indexes = random.sample(list_indexes,\n",
        "                                           k=min(len(list_indexes), batch_size))\n",
        "            list_indexes = list(filter(lambda x: x not in set(random_indexes), list_indexes))\n",
        "\n",
        "            gradient = sum((2 * X[index] * (X[index].dot(W) - Y[index])) / len(random_indexes) for index in random_indexes)\n",
        "            W = W - h * gradient\n",
        "            cnt_iter += 1\n",
        "            h = learning_rate_function(h, attenuation_parameter, cnt_iter)\n",
        "\n",
        "            current_error = (Y[i] - np.dot(W, X[i])) ** 2\n",
        "            Q_new = forgetting_rate * current_error + (1 - forgetting_rate) * Q_new\n",
        "\n",
        "        if abs(Q_new - Q_last) < eps:\n",
        "            break\n",
        "\n",
        "        Q_last = Q_new\n",
        "        cnt_epoch += 1\n",
        "\n",
        "    return [cnt_epoch, cnt_iter, W, Q_new]\n",
        "\n",
        "def constant_learning_rate_function(initial_value_learning_rate: float, attenuation_parameter: float, n: int) -> float:\n",
        "  return initial_value_learning_rate\n",
        "\n",
        "def time_learning_rate_function(initial_value_learning_rate: float, attenuation_parameter: float, n: int) -> float:\n",
        "  return initial_value_learning_rate / (1 + attenuation_parameter * n)\n",
        "\n",
        "def exponential_learning_rate_function(initial_value_learning_rate: float, attenuation_parameter: float, n: int) -> float:\n",
        "  return initial_value_learning_rate * math.exp(-attenuation_parameter)\n",
        "\n",
        "\n",
        "def stochastic_gradient_descent_with_constant_learning_rate(X: np.array,\n",
        "                                Y: np.array,\n",
        "                                batch_size: int = 1,\n",
        "                                learning_rate: float = 0.01,\n",
        "                                forgetting_rate: float = 0.1,\n",
        "                                eps: float = 1e-6):\n",
        "  return stochastic_gradient_descent(X, Y, forgetting_rate, constant_learning_rate_function, learning_rate, 1, batch_size, eps)\n",
        "\n",
        "def stochastic_gradient_descent_with_time_learning_rate(X: np.array,\n",
        "                                Y: np.array,\n",
        "                                batch_size: int = 1,\n",
        "                                initial_value_learning_rate: float = 0.1,\n",
        "                                attenuation_parameter: float = 0.5,\n",
        "                                forgetting_rate: float = 0.1,\n",
        "                                eps: float = 1e-6):\n",
        "  return stochastic_gradient_descent(X, Y, forgetting_rate, time_learning_rate_function, initial_value_learning_rate, attenuation_parameter, batch_size, eps)\n",
        "\n",
        "def stochastic_gradient_descent_with_exponential_learning_rate(X: np.array,\n",
        "                                Y: np.array,\n",
        "                                batch_size: int = 1,\n",
        "                                initial_value_learning_rate: float = 0.1,\n",
        "                                attenuation_parameter: float = 0.5,\n",
        "                                forgetting_rate: float = 0.1,\n",
        "                                eps: float = 1e-6):\n",
        "  return stochastic_gradient_descent(X, Y, forgetting_rate, exponential_learning_rate_function, initial_value_learning_rate, attenuation_parameter, batch_size, eps)\n"
      ],
      "metadata": {
        "id": "eqwfsbEe7TXM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача:**    \n",
        "Реализовать и исследовать на эффективность SGD для полиномиальной\n",
        "регрессии с добавлением регуляризации в модель разных методов\n",
        "регуляризации (L1, L2, Elastic регуляризации).\n",
        "\n",
        "**О регуляризации:**   \n",
        "Один из способов бороться с негативным эффектом излишнего подстраивания под данные — использование регуляризации, т. е. добавление некоторого штрафа за большие значения коэффициентов в модели. Тем самым запрещаются слишком \"резкие\" изгибы, и предотвращается переобучение.\n",
        "1. L1-регуляризация    \n",
        "$R(w) = \\sum_{i=1}^N |b_i| $\n",
        "2. L2-регуляризация    \n",
        "$R(w) = \\sum_{i=1}^N (b_i)^2 $\n",
        "3. Elastic регуляризация    \n",
        "$R(w) = \\lambda_1 \\cdot \\sum_{i=1}^N |b_i| + \\lambda_2 \\cdot \\sum_{i=1}^N (b_i)^2 $\n",
        "\n",
        "**Условия реализации полиномиальной регрессии:**\n",
        "1. $y$ может зависеть от каждой независимой переменной $x_k$ в виде степени, значение которой может быть от 1 до N\n",
        "2. N = 9\n",
        "3. Понятно, что на одинаковых входных данных разные функции $g1$ и $g2$ могут давать равные с некоторой точностью ответы. Так что написанный метод возвращает три ответа, у которых наименьшее значение функции ошибки.\n",
        "\n"
      ],
      "metadata": {
        "id": "HP8zZNb88Sm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Доп 1 - полиномы\n",
        "\n",
        "def polynom_stochastic_gradient_descent(X: np.array,\n",
        "                                Y: np.array,\n",
        "                                forgetting_rate: float = 0.1,\n",
        "                                learning_rate: float = 0.01,\n",
        "                                batch_size: int = 1,\n",
        "                                eps: float = 1e-6,\n",
        "                                ):\n",
        "\n",
        "    data_size, N = X.shape\n",
        "\n",
        "    result_W = np.array([np.zeros(N), np.zeros(N), np.zeros(N)])\n",
        "    result_D = np.zeros(3)\n",
        "    result_cnt_epoch = np.zeros(3)\n",
        "    result_cnt_iter = np.zeros(3)\n",
        "    result_Q = np.array([10e9, 10e9, 10e9])\n",
        "\n",
        "    for permutation in itertools.product('123456789', repeat=N):\n",
        "      D = ''.join(permutation)\n",
        "\n",
        "      W = np.zeros(N)\n",
        "\n",
        "      Q_last = sum((Y[i] - sum(W[j] * pow(X[i][j], int(D[j])) for j in range(N))) ** 2 / data_size for i in range(data_size))\n",
        "      Q_new = Q_last\n",
        "\n",
        "      cnt_epoch = 0\n",
        "      cnt_iter = 0\n",
        "\n",
        "      while True:\n",
        "        list_indexes = list(range(0, data_size))\n",
        "\n",
        "        for i in range(math.ceil(data_size / batch_size)):\n",
        "            random_indexes = random.sample(list_indexes,\n",
        "                                           k=min(len(list_indexes), batch_size))\n",
        "            list_indexes = list(filter(lambda x: x not in set(random_indexes), list_indexes))\n",
        "\n",
        "            gradient = sum((2 * X[index] * (sum(W[j] * pow(X[index][j], int(D[j])) for j in range(N)) - Y[index])) / len(random_indexes) for index in random_indexes)\n",
        "            W = W - learning_rate * gradient\n",
        "            cnt_iter += 1\n",
        "\n",
        "            current_error = (Y[i] - sum(W[j] * pow(X[i][j], int(D[j])) for j in range(N))) ** 2\n",
        "            Q_new = forgetting_rate * current_error + (1 - forgetting_rate) * Q_new\n",
        "\n",
        "        if abs(Q_new - Q_last) < eps:\n",
        "            break\n",
        "\n",
        "        Q_last = Q_new\n",
        "        cnt_epoch += 1\n",
        "\n",
        "      max_Q = result_Q[0]\n",
        "      k_max_Q = 0\n",
        "      for k in range(3):\n",
        "        if result_Q[k] > max_Q:\n",
        "          max_Q = result_Q[k]\n",
        "          k_max_Q = k\n",
        "\n",
        "      if (Q_new < max_Q):\n",
        "        print(D)\n",
        "        result_W[k_max_Q] = W\n",
        "        result_D[k_max_Q] = int(D)\n",
        "        result_cnt_epoch[k_max_Q] = cnt_epoch\n",
        "        result_cnt_iter[k_max_Q] = cnt_iter\n",
        "        result_Q[k_max_Q] = Q_new\n",
        "\n",
        "    return [result_cnt_epoch, result_cnt_iter, result_W, result_D, result_Q]\n",
        "\n",
        "\n",
        "# Доп1 - регуляризация\n",
        "\n",
        "def stochastic_gradient_descent_with_regularization(X: np.array,\n",
        "                                Y: np.array,\n",
        "                                regularization: Callable[[np.array], float],\n",
        "                                forgetting_rate: float = 0.1,\n",
        "                                learning_rate: float = 0.01,\n",
        "                                batch_size: int = 1,\n",
        "                                eps: float = 1e-6,\n",
        "                                ):\n",
        "    data_size, number_coefficients = X.shape\n",
        "    W = np.zeros(number_coefficients)\n",
        "    #W = np.full(shape=number_coefficients, fill_value=500)\n",
        "\n",
        "    Q_last = sum(((Y[i] - np.dot(W, X[i])) ** 2) / data_size for i in range(data_size))\n",
        "    Q_new = Q_last\n",
        "\n",
        "    cnt_epoch = 0\n",
        "    cnt_iter = 0\n",
        "    while True:\n",
        "        list_indexes = list(range(0, data_size))\n",
        "\n",
        "        for i in range(math.ceil(data_size / batch_size)):\n",
        "            random_indexes = random.sample(list_indexes,\n",
        "                                           k=min(len(list_indexes), batch_size))\n",
        "            list_indexes = list(filter(lambda x: x not in set(random_indexes), list_indexes))\n",
        "\n",
        "            gradient = sum((2 * X[index] * (X[index].dot(W) - Y[index])) / len(random_indexes) for index in random_indexes)\n",
        "            W = W - learning_rate * gradient\n",
        "            cnt_iter += 1\n",
        "\n",
        "            current_error = (Y[i] - np.dot(W, X[i])) ** 2 + regularization(W)\n",
        "            Q_new = forgetting_rate * current_error + (1 - forgetting_rate) * Q_new\n",
        "\n",
        "        if abs(Q_new - Q_last) < eps:\n",
        "            break\n",
        "\n",
        "        Q_last = Q_new\n",
        "        cnt_epoch += 1\n",
        "\n",
        "    return [cnt_epoch, cnt_iter, W, Q_new]\n",
        "\n",
        "def L1(W: np.array) -> float:\n",
        "  return sum(abs(W[i]) for i in range(len(W)))\n",
        "\n",
        "def L2(W: np.array) -> float:\n",
        "  return sum(pow(W[i], 2) for i in range(len(W)))\n",
        "\n",
        "def Elastic(W: np.array) -> float:\n",
        "  return sum(0.5 * abs(W[i]) + 0.5 * pow(W[i], 2) for i in range(len(W)))"
      ],
      "metadata": {
        "id": "9m5W4dsh8SH5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тесты"
      ],
      "metadata": {
        "id": "KFLCNREU-wIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Тесты\n",
        "\n",
        "# Для размеров батчей (learning_rate = 0.01, forgetting_rate = 0.1)\n",
        "\n",
        "def print_result_of_batch_test(batch_size, data):\n",
        "  theta = stochastic_gradient_descent_with_constant_learning_rate(data[0], data[1])\n",
        "\n",
        "  print(\"Batch-size = \", batch_size, \":\")\n",
        "  print(\"Count_epoch : \", theta[0], \" count_iterations : \", theta[1])\n",
        "  print(\"Result : \", theta[2])\n",
        "\n",
        "def one_batch_test(input_coefficients, count_data, batch_size1, batch_size2, batch_size3):\n",
        "  data = data_fabric(count_data, input_coefficients, np.ones((len(input_coefficients))))\n",
        "  print(\"Count_data : \", count_data, \"input_coefficients : \", input_coefficients)\n",
        "  print_result_of_batch_test(batch_size1, data)\n",
        "  print_result_of_batch_test(batch_size2, data)\n",
        "  print_result_of_batch_test(batch_size3, data)\n",
        "  print(\"----------------------------------------------\")\n",
        "  print()\n",
        "\n",
        "def test_batches_standart_SGD():\n",
        "  random_input_coefficients1 = np.array(list(np.random.randint(-10, 10, 6)))\n",
        "  one_batch_test(random_input_coefficients1, 10, 1, 3, 6)\n",
        "\n",
        "  random_input_coefficients2 = np.array(list(np.random.randint(-10, 10, 6)))\n",
        "  one_batch_test(random_input_coefficients2, 100, 1, 3, 6)\n",
        "\n",
        "  many_input_coefficients = [i for i in range(100)]\n",
        "  one_batch_test(many_input_coefficients, 50, 1, 50, 100)\n",
        "\n",
        "# Для разных функций изменения шага\n",
        "\n",
        "def print_result_of_test(theta):\n",
        "  print(\"Count_epoch : \", theta[0], \" count_iterations : \", theta[1])\n",
        "  print(\"Result : \", theta[2])\n",
        "\n",
        "def test_constant_learning_rate(data, learning_rate: np.array):\n",
        "  for i in range(len(learning_rate)):\n",
        "    print()\n",
        "    coefficient = learning_rate[i]\n",
        "    theta = stochastic_gradient_descent_with_constant_learning_rate(data[0], data[1], learning_rate=coefficient)\n",
        "    print(\"initial_value_learning_rate = \", coefficient)\n",
        "    print_result_of_test(theta)\n",
        "  print(\"----------------------------------------------------------------------------------------------------\")\n",
        "  print()\n",
        "\n",
        "def test_NON_constant_learning_rate(data, SGD_function, initial_value_learning_rate: np.array, attenuation_parameter: np.array):\n",
        "  for i in range(len(initial_value_learning_rate)):\n",
        "    for j in range(len(attenuation_parameter)):\n",
        "      coefficient = initial_value_learning_rate[i]\n",
        "      parameter = attenuation_parameter[j]\n",
        "      print()\n",
        "      print(\"initial_value_learning_rate = \", coefficient, \" attenuation_parameter = \", parameter)\n",
        "      theta = SGD_function(data[0], data[1], initial_value_learning_rate=coefficient, attenuation_parameter=parameter)\n",
        "      print_result_of_test(theta)\n",
        "  print(\"----------------------------------------------------------------------------------------------------\")\n",
        "  print()\n",
        "\n",
        "def test_learning_rate():\n",
        "  input_coefficients = np.array(list(np.random.randint(-10, 10, 10)))\n",
        "  print(\"input_coefficients : \", input_coefficients)\n",
        "  print(\"----------------------------------------------------------------------------------------------------\")\n",
        "  print()\n",
        "  data = data_fabric(20, input_coefficients, np.ones((len(input_coefficients))))\n",
        "\n",
        "  print(\"Constant learning rate\")\n",
        "  test_constant_learning_rate(data, np.array([0.01, 0.05, 0.1]))\n",
        "\n",
        "  print(\"Time learning rate\")\n",
        "  test_NON_constant_learning_rate(data, stochastic_gradient_descent_with_time_learning_rate, np.array([1, 0.2]), np.array([1, 0.1, 0.01]))\n",
        "\n",
        "  print(\"Exponential learning rate\")\n",
        "  test_NON_constant_learning_rate(data, stochastic_gradient_descent_with_exponential_learning_rate, np.array([1, 0.2]), np.array([1, 0.1, 0.01]))\n",
        "\n",
        "# Тест для полинома\n",
        "\n",
        "def polynom_test():\n",
        "  data = data_fabric(20, np.array([142, 201, -56]), np.array([9, 2, 4]))\n",
        "\n",
        "  R = polynom_stochastic_gradient_descent(data[0], data[1], eps=1e-2)\n",
        "  for k in range(3):\n",
        "    print(\"Coefficients : \", R[2][k], \" degrees : [\", ' '.join(str(int(R[3][k]))), \"] error : \", R[4][k])\n",
        "\n",
        "# Тест для регуляризации\n",
        "\n",
        "def regularization_test():\n",
        "  input_coefficients = list([0, 0, 0, 5, 0, 0, 7, 0, 0, 0])\n",
        "  #np.array(list(np.random.randint(-10, 10, 10)))\n",
        "  print(\"input_coefficients : \", input_coefficients)\n",
        "  print()\n",
        "  data = data_fabric(20, input_coefficients, np.ones((len(input_coefficients))))\n",
        "\n",
        "  theta = stochastic_gradient_descent_with_constant_learning_rate(data[0], data[1])\n",
        "  print(\"Default\")\n",
        "  print_result_of_test(theta)\n",
        "  theta_l1 = stochastic_gradient_descent_with_regularization(data[0], data[1], L1)\n",
        "  print()\n",
        "  print(\"L1\")\n",
        "  print_result_of_test(theta_l1)\n",
        "  theta_l2 = stochastic_gradient_descent_with_regularization(data[0], data[1], L2)\n",
        "  print()\n",
        "  print(\"L2\")\n",
        "  print_result_of_test(theta_l2)\n",
        "  theta_elastic = stochastic_gradient_descent_with_regularization(data[0], data[1], Elastic)\n",
        "  print()\n",
        "  print(\"Elastic\")\n",
        "  print_result_of_test(theta_elastic)\n",
        "\n",
        "# Для сравнения с библиотечными реализациями\n",
        "last_epoch = 0\n",
        "\n",
        "def test_library(count_coefficient=10, count_data=100, batch_size=32, eps1=1e-7):\n",
        "    global last_epoch\n",
        "\n",
        "    class LossCallback(Callback):\n",
        "        eps = eps1\n",
        "        prev = 0.0\n",
        "\n",
        "        def on_batch_end(self, batch, logs=None):\n",
        "            if logs['loss'] is None:\n",
        "                return\n",
        "            elif abs(self.prev - logs['loss']) < self.eps:\n",
        "\n",
        "                self.model.stop_training = True\n",
        "            self.prev = logs['loss']\n",
        "\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            global last_epoch\n",
        "            last_epoch = epoch\n",
        "\n",
        "    optimizers = {\n",
        "        \"adagrad\": tf.keras.optimizers.Adagrad(learning_rate=0.8, initial_accumulator_value=0.05),\n",
        "        \"nesterov\": tf.keras.optimizers.SGD(learning_rate=0.07, nesterov=True, momentum=0.6),\n",
        "        \"sgd\": tf.keras.optimizers.SGD(learning_rate=0.08),\n",
        "        \"rmsprop\": tf.keras.optimizers.RMSprop(learning_rate=0.08, rho=0.2, momentum=0.1),\n",
        "        \"adam\": tf.keras.optimizers.Adam(learning_rate=0.07, beta_1=0.9, beta_2=0.99),\n",
        "        \"momentum\": tf.keras.optimizers.SGD(learning_rate=0.07, momentum=0.4),\n",
        "    }\n",
        "    input_coefficients = np.array(list(np.random.randint(-30, 30, count_coefficient)))\n",
        "    print(\"input_coefficients : \", input_coefficients)\n",
        "    print(\"----------------------------------------------------------------------------------------------------\")\n",
        "    data = data_fabric(count_data, input_coefficients, np.ones((len(input_coefficients))))\n",
        "    X, y = data[0], data[1]\n",
        "\n",
        "    for method in optimizers.keys():\n",
        "        print(method)\n",
        "        model = Sequential([\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=optimizers[method], loss='mean_squared_error')\n",
        "        model.fit(X, y, batch_size=batch_size, epochs=10000, verbose=0,\n",
        "                  callbacks=[LossCallback()])\n",
        "\n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(count_coefficient):\n",
        "            print(model.layers[0].weights[0][i][0].numpy(), end=\", \")\n",
        "        print(\"]\")\n",
        "        print(f\"Epochs: {last_epoch}, iterations: {last_epoch * batch_size + 1}\")\n",
        "        last_epoch = 0\n",
        "\n",
        "\n",
        "# Main\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "  #regularization_test()\n"
      ],
      "metadata": {
        "id": "UkQ1UUJ4-yq-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_batches_standart_SGD()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGTWC57Y_PBy",
        "outputId": "1a3e79bf-6a69-48d9-a747-6d93ffb7f684"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count_data :  10 input_coefficients :  [-2 -9 -4 -7 -3  7]\n",
            "Batch-size =  1 :\n",
            "Count_epoch :  366  count_iterations :  3670\n",
            "Result :  [-2.08594145 -8.57050025 -4.47411671 -6.92920355 -2.74022366  6.8019926 ]\n",
            "Batch-size =  3 :\n",
            "Count_epoch :  518  count_iterations :  5190\n",
            "Result :  [-2.04245383 -8.80946979 -4.20323987 -6.97816654 -2.89603962  6.92679078]\n",
            "Batch-size =  6 :\n",
            "Count_epoch :  599  count_iterations :  6000\n",
            "Result :  [-2.02714258 -8.87789528 -4.12836313 -6.98699715 -2.93485138  6.95575355]\n",
            "----------------------------------------------\n",
            "\n",
            "Count_data :  100 input_coefficients :  [ 7  2 -1 -8  7  3]\n",
            "Batch-size =  1 :\n",
            "Count_epoch :  55  count_iterations :  5600\n",
            "Result :  [ 7.00248662  2.0026859  -0.99729798 -7.99674867  6.99325057  2.99536441]\n",
            "Batch-size =  3 :\n",
            "Count_epoch :  55  count_iterations :  5600\n",
            "Result :  [ 7.00263105  2.00274245 -0.9972052  -7.99663846  6.99316118  2.99533395]\n",
            "Batch-size =  6 :\n",
            "Count_epoch :  55  count_iterations :  5600\n",
            "Result :  [ 7.00258238  2.00277152 -0.99725887 -7.99671018  6.99323177  2.99543559]\n",
            "----------------------------------------------\n",
            "\n",
            "Count_data :  50 input_coefficients :  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "Batch-size =  1 :\n",
            "Count_epoch :  249  count_iterations :  12500\n",
            "Result :  [  8.80445423  24.22976849  25.55477826  32.72845256  19.46712643\n",
            "  23.86380143  29.46735493  10.5266024   29.6578965    7.00189254\n",
            "  30.62690431  50.87327877  59.34066918  27.64664525  29.00503759\n",
            "  20.01637715  40.00447055  54.72167042  24.36840992  36.13120869\n",
            "  48.16824295  46.98537195  39.58438534  30.22145553  45.29360562\n",
            "  43.64689311  14.55225884  36.24473938  40.7641248   20.6113431\n",
            "  48.99945948  38.51119634  52.38419199  52.57632029  39.43944721\n",
            "  31.46099695  46.15840172  45.18251653  59.71495381  19.07742763\n",
            "  38.23763125  69.95924467  22.77517216  17.83551619  66.45740922\n",
            "  32.60369763  56.60508742  73.57566278  40.78870086  46.64796155\n",
            "  44.45672329  52.17530697  31.28546689  45.43346326  45.23583302\n",
            "  55.952908    35.25682693  38.1365676   46.27108797  61.94001445\n",
            "  52.37105963  56.83927723  65.73869756  27.81382355  69.68711846\n",
            "  58.91801928  35.07800284  61.04703258  51.72009496  30.97790342\n",
            "  43.22766472  51.11799262  56.76908805  77.86104442  53.35124621\n",
            "  77.35315177  73.82871062  68.5894382   85.08798694  59.0856175\n",
            "  80.46630401  59.2174941   93.95068313  48.31386722  79.29115141\n",
            "  85.13654844  59.58791784  90.58566254  76.94893192  87.02311721\n",
            "  57.40532461  65.07413506  48.08115252  85.1893444   59.76463318\n",
            " 106.84387404  67.95572335  95.35982004  52.6346538   90.06658712]\n",
            "Batch-size =  50 :\n",
            "Count_epoch :  368  count_iterations :  18450\n",
            "Result :  [  8.7775228   24.21666955  25.54060341  32.71206197  19.49622183\n",
            "  23.85734024  29.45444087  10.50940531  29.64487064   6.95842821\n",
            "  30.59697966  50.87629849  59.398633    27.70278735  29.01191255\n",
            "  19.97365552  40.04754308  54.68741652  24.33451509  36.09990109\n",
            "  48.11563618  46.98517039  39.57398346  30.1821328   45.32329688\n",
            "  43.61695382  14.52467788  36.2594551   40.76629521  20.62946073\n",
            "  48.99351836  38.46538855  52.38673914  52.60274166  39.46326475\n",
            "  31.40736093  46.08738302  45.23130415  59.67122387  19.04386335\n",
            "  38.23014189  69.92376042  22.67222892  17.81022903  66.55191209\n",
            "  32.55862917  56.65204992  73.57471711  40.77853601  46.71265713\n",
            "  44.43574662  52.20144313  31.3233999   45.48268177  45.2397896\n",
            "  55.92195887  35.29872758  38.14357894  46.29263472  61.93793437\n",
            "  52.46282743  56.86687768  65.74850855  27.82134799  69.65085223\n",
            "  58.91686424  35.07598977  61.01545456  51.74260434  30.94963392\n",
            "  43.14757718  51.15275577  56.76579556  77.85568079  53.28578255\n",
            "  77.39043509  73.75468643  68.54611124  85.07628787  59.08996355\n",
            "  80.52256969  59.28157872  93.94086442  48.32161952  79.42514541\n",
            "  85.15372761  59.56103846  90.6602051   76.92267653  86.99561502\n",
            "  57.38225898  65.07417994  48.06524251  85.18261633  59.80624145\n",
            " 106.90902086  68.0129831   95.32451037  52.68503212  90.07691587]\n",
            "Batch-size =  100 :\n",
            "Count_epoch :  403  count_iterations :  20200\n",
            "Result :  [  8.77567268  24.21615215  25.54130256  32.71051008  19.49709746\n",
            "  23.85782856  29.45436715  10.50882008  29.64326842   6.95722194\n",
            "  30.5957567   50.87674942  59.4004762   27.70453856  29.01199368\n",
            "  19.9726135   40.04924109  54.68694907  24.33256307  36.0990325\n",
            "  48.11303808  46.98516267  39.57341826  30.18072887  45.32497906\n",
            "  43.61739706  14.52342998  36.25958818  40.76661566  20.63015238\n",
            "  48.99300223  38.46383629  52.38661821  52.60286373  39.46456745\n",
            "  31.4057017   46.08482948  45.23302694  59.66877255  19.04341249\n",
            "  38.22958742  69.92201826  22.66869021  17.80941871  66.5552346\n",
            "  32.55689085  56.65242755  73.57449722  40.77785175  46.71570693\n",
            "  44.43493493  52.20158147  31.32472383  45.4852177   45.2394419\n",
            "  55.92097537  35.30039954  38.14308455  46.29358261  61.93679874\n",
            "  52.4661054   56.86770925  65.74827756  27.82118086  69.6501923\n",
            "  58.91651794  35.07516479  61.01472811  51.74411827  30.94945693\n",
            "  43.14490765  51.15396702  56.76539369  77.85525064  53.28319587\n",
            "  77.39174409  73.7521195   68.5445177   85.07508654  59.08997093\n",
            "  80.52444293  59.28351347  93.93998388  48.32184605  79.42909881\n",
            "  85.15418225  59.55972113  90.66223517  76.92166745  86.99372823\n",
            "  57.38097902  65.07419563  48.06379933  85.18200389  59.80773208\n",
            " 106.91064053  68.01464036  95.32293164  52.68707498  90.07785029]\n",
            "----------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_learning_rate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwrHXPQA_jhv",
        "outputId": "5a6d8e18-31a4-4788-a732-d0a3c43c8f16"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_coefficients :  [-2  5  1  4  7 -2 -4  0 -4 -7]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Constant learning rate\n",
            "\n",
            "initial_value_learning_rate =  0.01\n",
            "Count_epoch :  408  count_iterations :  8180\n",
            "Result :  [-1.89632457  4.75572201  1.00723259  4.14238938  7.04318151 -2.22887587\n",
            " -3.96507664 -0.17455826 -3.79369482 -6.87943426]\n",
            "\n",
            "initial_value_learning_rate =  0.05\n",
            "Count_epoch :  109  count_iterations :  2200\n",
            "Result :  [-1.9540273   4.89932037  0.99536601  4.06574658  7.0200911  -2.09486572\n",
            " -3.98633306 -0.0659529  -3.91511394 -6.94654103]\n",
            "\n",
            "initial_value_learning_rate =  0.1\n",
            "Count_epoch :  91  count_iterations :  1840\n",
            "Result :  [-1.99160545  4.98262079  0.99908705  4.00887318  7.00298209 -2.01447033\n",
            " -3.99903665 -0.01221307 -3.98637984 -6.99228384]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Time learning rate\n",
            "\n",
            "initial_value_learning_rate =  1.0  attenuation_parameter =  1.0\n",
            "Count_epoch :  10  count_iterations :  220\n",
            "Result :  [ 17.21277436  -2.22835628  -4.3332756    0.16578851  11.474149\n",
            "  12.05188134 -24.19667114 -15.16123965 -12.08449126 -13.65699584]\n",
            "\n",
            "initial_value_learning_rate =  1.0  attenuation_parameter =  0.1\n",
            "Count_epoch :  13  count_iterations :  280\n",
            "Result :  [ 327.56026654 -156.36445824   -6.25263412 -116.18419896 -165.22281822\n",
            "  318.79973299  -29.35792669   65.39864744 -204.59804636   85.03808114]\n",
            "\n",
            "initial_value_learning_rate =  1.0  attenuation_parameter =  0.01\n",
            "Count_epoch :  19  count_iterations :  400\n",
            "Result :  [  35835.49262619  165153.45005984   78069.3788285    71494.0033937\n",
            "  -73235.11954716 -140167.78657665     796.39004604  -50552.27514858\n",
            "  -71683.44327287  -49483.4310771 ]\n",
            "\n",
            "initial_value_learning_rate =  0.2  attenuation_parameter =  1.0\n",
            "Count_epoch :  7  count_iterations :  160\n",
            "Result :  [-1.70776477 -0.54398491  1.03849701  0.02494149  1.17132276 -0.64273463\n",
            " -0.15251712 -2.67516998 -1.96376202 -2.09373462]\n",
            "\n",
            "initial_value_learning_rate =  0.2  attenuation_parameter =  0.1\n",
            "Count_epoch :  8  count_iterations :  180\n",
            "Result :  [-0.7537643   1.32184866  0.50509609  0.569882    2.10495547 -1.4100302\n",
            "  0.72831483 -0.36902398 -0.20975582 -1.5568096 ]\n",
            "\n",
            "initial_value_learning_rate =  0.2  attenuation_parameter =  0.01\n",
            "Count_epoch :  9  count_iterations :  200\n",
            "Result :  [-1.03965186e+00  1.12830408e+00  2.32084465e+00  2.72049050e+00\n",
            "  2.55237799e+00 -1.74236252e+00  1.97822477e-03 -1.38559454e+00\n",
            " -3.99022299e+00 -2.52579274e+00]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Exponential learning rate\n",
            "\n",
            "initial_value_learning_rate =  1.0  attenuation_parameter =  1.0\n",
            "Count_epoch :  9  count_iterations :  200\n",
            "Result :  [-7.39371375 -1.53775528 -8.91833805 -2.40592107 -1.94104275  2.44982621\n",
            " -4.72335571 -0.53350311  6.46104971  3.4152575 ]\n",
            "\n",
            "initial_value_learning_rate =  1.0  attenuation_parameter =  0.1\n",
            "Count_epoch :  16  count_iterations :  340\n",
            "Result :  [  880.25984069   656.44576723   -68.66078975   126.5112308\n",
            "   557.12983557 -1193.43314098   727.62027176  -151.09520902\n",
            "  -997.04501373  -766.76055352]\n",
            "\n",
            "initial_value_learning_rate =  1.0  attenuation_parameter =  0.01\n",
            "Count_epoch :  191  count_iterations :  3840\n",
            "Result :  [-4.22542514e+27 -1.36690157e+27  1.03088413e+28 -9.38755996e+27\n",
            "  5.74299716e+27 -1.37679193e+27 -8.96626417e+26 -5.80481090e+27\n",
            "  1.00149416e+28 -1.85949791e+27]\n",
            "\n",
            "initial_value_learning_rate =  0.2  attenuation_parameter =  1.0\n",
            "Count_epoch :  7  count_iterations :  160\n",
            "Result :  [ 0.26061053  0.13887493 -0.0272407   0.2187865   0.58762143  0.29924004\n",
            "  0.4689216  -0.03558103 -0.51041523 -0.01044517]\n",
            "\n",
            "initial_value_learning_rate =  0.2  attenuation_parameter =  0.1\n",
            "Count_epoch :  10  count_iterations :  220\n",
            "Result :  [ 0.33988897  0.98121412  1.0723701   1.96113523  2.43173527 -0.57766263\n",
            "  0.11101148 -2.65217129 -2.61698156 -3.23740173]\n",
            "\n",
            "initial_value_learning_rate =  0.2  attenuation_parameter =  0.01\n",
            "Count_epoch :  48  count_iterations :  980\n",
            "Result :  [-1.83878662  3.99695519  1.47451014  4.28952126  6.34441344 -2.08478596\n",
            " -3.33768451 -0.7027265  -3.74585895 -6.25715864]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regularization_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-LIulSL_nA7",
        "outputId": "dc6d144a-2fd7-41da-95c3-fe44a024bf91"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_coefficients :  [0, 0, 0, 5, 0, 0, 7, 0, 0, 0]\n",
            "\n",
            "Default\n",
            "Count_epoch :  221  count_iterations :  4440\n",
            "Result :  [ 5.72533692e-02  2.22549572e-03  8.70864097e-03  4.97960038e+00\n",
            "  5.67219810e-02 -2.92900281e-02  6.88218394e+00 -5.88034175e-04\n",
            "  3.10023323e-02  1.62649291e-02]\n",
            "\n",
            "L1\n",
            "Count_epoch :  512  count_iterations :  10260\n",
            "Result :  [-7.59426148e-04  3.90292901e-03  2.19467799e-04  5.00411701e+00\n",
            "  2.48152179e-03 -3.07222173e-03  6.99594133e+00 -4.90567966e-03\n",
            "  3.92938420e-04 -7.68869836e-06]\n",
            "\n",
            "L2\n",
            "Count_epoch :  771  count_iterations :  15440\n",
            "Result :  [-4.19049433e-04  1.04807141e-03  7.34706078e-05  5.00106614e+00\n",
            "  5.33763185e-04 -5.46902261e-04  6.99919280e+00 -1.14937835e-03\n",
            " -4.44698338e-05 -1.50027871e-04]\n",
            "\n",
            "Elastic\n",
            "Count_epoch :  717  count_iterations :  14360\n",
            "Result :  [-5.49871005e-04  1.41568518e-03  9.77262176e-05  5.00146595e+00\n",
            "  7.37059522e-04 -7.67097179e-04  6.99888238e+00 -1.57635441e-03\n",
            " -3.89289724e-05 -1.93538688e-04]\n"
          ]
        }
      ]
    }
  ]
}