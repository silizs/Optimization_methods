{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNA5rNHZYqjFpiUhG0x7FCI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSWRs0LOh5NM"
      },
      "outputs": [],
      "source": [
        "import scipy as sp\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Метод градиентного спуска\n",
        "Метод градиентного спуска позволяет искать локальный минимум функции нескольких переменных. Идея алгоритма заключается в том, чтобы на каждой итерации двигаться от текущей точки в направлении убывания, то есть в направлении, противоположном градиенту функции: $s_{(k)} = -grad(f(x_{(k)}))$.\n",
        "Однако необходимо найти коэффициент $coef$ с которым мы будем идти по этому направлению: $x_{(k + 1)}  = x_{(k)} + coef * s_{(k)}$.\n",
        "\n"
      ],
      "metadata": {
        "id": "capjGpMViYG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(f_runnable, x0, f_step, eps, step_size=0.0):\n",
        "    \"\"\"Gradient descent method.\n",
        "\n",
        "    Keyword arguments:\n",
        "    f_runnable -- objective function (for which we want to find the minimum)\n",
        "    x0 -- starting point\n",
        "    f_step -- method for finding the learning (step) rate\n",
        "    eps -- calculation accuracy.\n",
        "\n",
        "\n",
        "    Returns a numpy array containing:\n",
        "    the x coordinates (at which the minimum is reached),\n",
        "    the minimum value,\n",
        "    the number of iterations,\n",
        "    the number of gradient calculations,\n",
        "    the number of function calculations.\n",
        "    \"\"\"\n",
        "\n",
        "    x_res = x0\n",
        "    x_last = x0\n",
        "    cnt_grad = 0\n",
        "    cnt_func = 0\n",
        "    cnt_iter = 0\n",
        "    exception = ''\n",
        "\n",
        "    while True:\n",
        "      cnt_iter += 1\n",
        "\n",
        "      try:\n",
        "        gradient = sp.optimize.approx_fprime(x_last, f_runnable, eps)\n",
        "        if any(np.isnan(gradient[i]) for i in range(len(gradient))):\n",
        "          exception = 'Overflow'\n",
        "      except Exception as e:\n",
        "        exception = str(e)\n",
        "\n",
        "      if exception != '':\n",
        "        break\n",
        "      cnt_grad += 1\n",
        "\n",
        "      eval_step = f_step(step_size, x_last, f_runnable, gradient, eps)\n",
        "      step = eval_step[0]\n",
        "      cnt_func += eval_step[1]\n",
        "      x_new = [x - step * grad_i for x, grad_i in zip(x_last, gradient)]\n",
        "\n",
        "      cnt_func += 2\n",
        "      if abs(f_runnable(x_new) - f_runnable(x_last)) < eps:\n",
        "        break;\n",
        "\n",
        "      x_last = x_res\n",
        "      x_res = x_new\n",
        "\n",
        "    return np.array([*x_res, f_runnable(x_res), cnt_iter, cnt_grad, cnt_func, exception]);"
      ],
      "metadata": {
        "id": "7_gnSxwJjtzP"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Различные методы поиска коээфициента шага на каждой итерации (learning rate)\n",
        "\n",
        "1. Константный шаг, заданный в диапазоне от 0 до 1. Большие значения соответствуют большему шагу во время одной итерации, соответственно алгоритм будет работать быстрее (так как для поиска минимума потребуется меньше итераций), но ответ будет менее точный. Малые значения коэффициента соответствуют большей точности ответа, но потребуется так же и больше итераций для его поиска.\n",
        "\n",
        "2. Метод золотого сечения – метод поиска экстремума функции одной переменной, где на каждой итерации мы имеем две точки (при этом на на первой итерации и далее мы считаем только одну, вторая сохранена с прошлой итерации и делит уже новый отрезок в отношении золотого сечения), после из трех промежутков отбрасываем соответствующий левый или правый.\n",
        "3. Метод дихотомии."
      ],
      "metadata": {
        "id": "S-KVa_KowH1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def constant_step(step_size, x_last, f_runnable, gradient, eps):\n",
        "    \"\"\"The function returns a constant specified step.\"\"\"\n",
        "    return np.array([step_size, 0]);\n",
        "\n",
        "def gradient_descent_with_constant_step(f_runnable, x0, step_size, eps):\n",
        "    \"\"\"Gradient descent method with constant learning rate.\"\"\"\n",
        "    return gradient_descent(f_runnable, x0, constant_step, eps, step_size)\n",
        "\n",
        "def golden_ratio_step(step_size, x_last, f_runnable, gradient, eps):\n",
        "    \"\"\"The function returns the step calculated using the golden ratio method.\"\"\"\n",
        "    cnt_func = 0\n",
        "    phi = (1 + 5 ** 0.5) / 2\n",
        "\n",
        "    a = 0\n",
        "    b = 1\n",
        "    ml = b - (b - a) / phi\n",
        "    mr = a + (b - a) / phi\n",
        "    xl = [x - ml * grad_i for x, grad_i in zip(x_last, gradient)]\n",
        "    xr = [x - mr * grad_i for x, grad_i in zip(x_last, gradient)]\n",
        "    lvalue = f_runnable(xl)\n",
        "    rvalue = f_runnable(xr)\n",
        "    cnt_func += 2\n",
        "\n",
        "    while (b - a > eps):\n",
        "      if lvalue < rvalue:\n",
        "        b = mr\n",
        "        mr = ml\n",
        "        ml = b - (b - a) / phi\n",
        "        rvalue = lvalue\n",
        "        xr = xl\n",
        "        xl = [x - ml * grad_i for x, grad_i in zip(x_last, gradient)]\n",
        "        lvalue = f_runnable(xl)\n",
        "        cnt_func += 1\n",
        "      else:\n",
        "        a = ml\n",
        "        ml = mr\n",
        "        mr = a + (b - a) / phi\n",
        "        lvalue = rvalue\n",
        "        xl = xr\n",
        "        xr = [x - mr * grad_i for x, grad_i in zip(x_last, gradient)]\n",
        "        rvalue = f_runnable(xr)\n",
        "        cnt_func += 1\n",
        "\n",
        "    return np.array([a, cnt_func]);\n",
        "\n",
        "def gradient_descent_with_golden_ratio_step(f_runnable, x0, eps):\n",
        "    \"\"\"Gradient descent method with learning rate calculated using the golden ratio method.\"\"\"\n",
        "    return gradient_descent(f_runnable, x0, golden_ratio_step, eps)\n",
        "\n",
        "def descent_step(step_size, x_last, f_runnable, gradient, eps):\n",
        "    \"\"\"The function returns the step calculated using the descent half method.\"\"\"\n",
        "    cnt_func = 0\n",
        "    a = 0\n",
        "    b = 1\n",
        "\n",
        "    while (b - a > 3 * eps):\n",
        "      ml = (a + b) / 2 - eps\n",
        "      mr = (a + b) / 2 + eps\n",
        "      xl = [x - ml * grad_i for x, grad_i in zip(x_last, gradient)]\n",
        "      xr = [x - mr * grad_i for x, grad_i in zip(x_last, gradient)]\n",
        "      cnt_func+=2\n",
        "      if f_runnable(xl) <= f_runnable(xr):\n",
        "        b = mr\n",
        "      else:\n",
        "        a = ml\n",
        "\n",
        "    return np.array([a, cnt_func]);\n",
        "\n",
        "def gradient_descent_with_descent_half_step(f_runnable, x0, eps):\n",
        "    \"\"\"\"Gradient descent method with learning rate calculated using the descent method.\"\"\"\n",
        "    return gradient_descent(f_runnable, x0, descent_step, eps)"
      ],
      "metadata": {
        "id": "TiG-OQBfwfBe"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тесты\n",
        "\n",
        "В тестах для сравнения так же используется библиотечный метод Нелдера-Мида.\n",
        "Метод Нелдера-Мида берет симплекс (выпуклая оболочка n+1 точки в n-мерном пространстве ) и отражает ‘максимальную” точку (точку, значение функции от которой максимально из всех n+1 рассматриваемых значений функций) и либо переходит к новому набору, где вместо “максимальной” точки новая (если новая “меньше” всех остальных), либо сжимает по прямой отражения (до тех пор пока не найдется “меньшая” точка), иначе стягивается к “минимальной” точке из набора."
      ],
      "metadata": {
        "id": "RStktYOUyytI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def message(name, result, len_x):\n",
        "  print(name)\n",
        "  if result[len_x + 4] != '':\n",
        "    print(\"Search failed. Exception: \", result[len_x + 4])\n",
        "    print()\n",
        "    return\n",
        "  print(\"----- Argument values = [\", \", \".join(str(result[i]) for i in range(len_x)), \"]\")\n",
        "  print(\"----- Function result = \", result[len_x])\n",
        "  print(\"----- Iteration count = \", result[len_x + 1])\n",
        "  print(\"----- Gradient evaluation count = \", result[len_x + 2])\n",
        "  print(\"----- Function evaluation count = \", result[len_x + 3])\n",
        "  print()\n",
        "\n",
        "def test(start_point, func, const_step, eps):\n",
        "  len_x = len(start_point)\n",
        "\n",
        "  print(\"Starting point: \" + str(start_point))\n",
        "  print(\"Const step: \", const_step)\n",
        "  print(\"Required accuracy : \", eps)\n",
        "  print('------------------------------------------')\n",
        "  print()\n",
        "\n",
        "  Nelder_Mead_res = sp.optimize.minimize(func, start_point, method=\"Nelder-Mead\")\n",
        "  np_arr_Nelder_Mead_res = np.array([*Nelder_Mead_res.x, func(Nelder_Mead_res.x), str(Nelder_Mead_res.nit), 0, Nelder_Mead_res.nfev, ''])\n",
        "  message('Nelder-Mead:', np_arr_Nelder_Mead_res, len_x)\n",
        "\n",
        "  constant_gradient_res = gradient_descent_with_constant_step(func, start_point, const_step, eps)\n",
        "  message('Constant gradient:', constant_gradient_res, len_x)\n",
        "\n",
        "  golden_ratio_gradient_res = gradient_descent_with_golden_ratio_step(func, start_point, eps)\n",
        "  message('Golden ratio gradient:', golden_ratio_gradient_res, len_x)\n",
        "\n",
        "  half_gradient_res = gradient_descent_with_descent_half_step(func, start_point, eps)\n",
        "  message('Half gradient:', half_gradient_res, len_x)"
      ],
      "metadata": {
        "id": "IXUxXMCZ2KD0"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример тестирования\n",
        "\n",
        "def test_func(x):\n",
        "  return x[0] ** 4 + x[1] ** 2 - x[0] * x[1]\n",
        "\n",
        "test(np.array([50, -50]), test_func, 0.2, 1e-9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM7zmZykCzMp",
        "outputId": "144cfe50-dd2c-41d6-9f6d-db65490df590"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting point: [ 50 -50]\n",
            "Const step:  0.2\n",
            "Required accuracy :  1e-09\n",
            "------------------------------------------\n",
            "\n",
            "Nelder-Mead:\n",
            "----- Argument values = [ -0.3535149135682817, -0.17678290062083504 ]\n",
            "----- Function result =  -0.015624998612451008\n",
            "----- Iteration count =  59\n",
            "----- Gradient evaluation count =  0\n",
            "----- Function evaluation count =  112\n",
            "\n",
            "Constant gradient:\n",
            "Search failed. Exception:  Overflow\n",
            "\n",
            "Golden ratio gradient:\n",
            "----- Argument values = [ -0.35358702968067307, -0.1767935076541603 ]\n",
            "----- Function result =  -0.01562499943415202\n",
            "----- Iteration count =  41\n",
            "----- Gradient evaluation count =  41\n",
            "----- Function evaluation count =  1968.0\n",
            "\n",
            "Half gradient:\n",
            "----- Argument values = [ -0.35358897478003676, -0.17679698751933032 ]\n",
            "----- Function result =  -0.01562499936056845\n",
            "----- Iteration count =  41\n",
            "----- Gradient evaluation count =  41\n",
            "----- Function evaluation count =  2542.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-141-db01bf624696>:4: RuntimeWarning: overflow encountered in scalar power\n",
            "  return x[0] ** 4 + x[1] ** 2 - x[0] * x[1]\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/optimize/_numdiff.py:590: RuntimeWarning: invalid value encountered in subtract\n",
            "  df = fun(x) - f0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Выводы\n",
        "\n",
        "1. Во-первых, заметим главное преимущество, которое мы уже знали, метод Нелдера-Мида, в отличие от градиентного спуска, не использует производные функции и, следовательно, может использоваться с негладкими и/или зашумленным функциям. Метод может быть менее эффективным в задачах с большим количеством переменных или сложной функцией.\n",
        "2. Метод градиентного спуска может не сходится. Он может не сходится, например, в случае, когда используется постоянный шаг и его значение большое для конкретной функции. Например, в функции $f(x, y) = x^2 + 100y^2$ при значении $> 0.1$. Это связано с тем, что при таком шаге смещение может перескакивать область с меньшими значениями, не заходить в нее и зациклится\\застрять на границе, не найдя минимум.\n",
        "3. Градиентный спуск с постоянным шагом работает сильно хуже спуска на основе золотого сечения, как минимум на порядок. Это связано с тем, что использование золотого сечения позволяет подстраивать длину перемещения: в отдаленных от минимума окрестностях с быстрым возрастанием функции можно двигаться к минимум большими шагами, но при этом не пропустить этот минимум.\n",
        "4. Градиентный спуск с использованием золотого сечения вычисляет значение функции минимум на порядок больше раз в сравнении с количеством итераций. Градиентный спуск с постоянным шагом использует ровно $2*countIter$ вычислений функции и, следовательно, при удачно выбранном шаге может быть эффективнее спуска с использованием золотого сечения на сложно (=затратно) вычислимых функциях.\n",
        "5. Большой минус метода градиентного спуска с постоянным шагом – это необходимость настройки параметра шага, который  может быть сложно подобрать.\n",
        "6. Метод дихотомии и метод золотого сечения примерно одинаково работают, различие только в том, что в методе дихотомии сравниваются значения $f(middle + eps)$ и $f(middle - eps)$, тем самым рассматриваемый промежуток уменьшается в примерно 2 раза, в то время как в золотом сечении промежуток уменьшается в 1.618.. раз, но необходимо вычислять на каждом следующем шаге только одно значение функции, следовательно, и всего вычислений при использовании метода золотого сечения меньше.\n"
      ],
      "metadata": {
        "id": "sBBK0V5T2LMi"
      }
    }
  ]
}